---
title: "Breast Cancer Prediction with ML II"
subtitle: "CS-715"
author: "Howard Nguyen & Olubukola Jaji"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
geometry: margin=1in
biblio-style: apalike
documentclass: book
classoption: openany
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, message=FALSE, warning=FALSE)
```

## Load libraries and dataset

The data was collected from University of Wisconsin Hospitals. Below is a summary of the attributes taken from the UCI Machine Learning repository.
 - Sample code number id number.
 - Clump Thickness.
 - Uniformity of Cell Size.
 - Uniformity of Cell Shape.
 - Marginal Adhesion.
 - Single Epithelial Cell Size.
 - Bare Nuclei.
 - Bland Chromatin.
 - Normal Nucleoli.
 - Mitoses.
 - Class.
Although the test methodologies differ, the best published results appear to be in the high 90% accuracy such as 96% and 97%. Achieving results in this range would be desirable in this case study.

```{r}
# load packages
library(mlbench)
library(caret)
library(corrplot)
# Load data
data(BreastCancer)
```

## Validation dataset

```{r}
# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validationIndex <- createDataPartition(BreastCancer$Class, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- BreastCancer[-validationIndex,]
# use the remaining 80% of data to training and testing the models
dataset <- BreastCancer[validationIndex,]
```

## Explore the dataset

```{r}
dim(dataset)
```
```{r}
head(dataset, n = 10)
```

```{r}
# Types
sapply(dataset, class)
```

```{r}
# Remove redundant variable Id
dataset <- dataset[,-1]
# convert input values to numeric
for(i in 1:9) {
dataset[,i] <- as.numeric(as.character(dataset[,i]))
}
```

```{r}
summary(dataset)
```
Interestingly, we can see we have 12 NA values for the Bare.nuclei attribute. This suggests we may need to remove the records (or impute values) with NA values for some analysis and modeling techniques. We can also see that all attributes have integer values in the range [1,10].
This suggests that we may not see much benefit form normalizing attributes for instance based methods like KNN.

```{r}
# class distribution
cbind(freq=table(dataset$Class), percentage=prop.table(table(dataset$Class))*100)
```


```{r}
# summarize correlations between input variables
complete_cases <- complete.cases(dataset)
cor(dataset[complete_cases,1:9])
```
We can see some modest to high correlation between some of the attributes. For example between cell shape and cell size at 0.90 correlation. Some algorithms may benefit from removing the highly correlated attributes.

```{r}
# visualized correlations between attributes
dataset_noNA <- na.omit(dataset)
correlationMatrix <- cor(dataset_noNA[,1:9])
corrplot(correlationMatrix, 
         order = "original", # order for labels, can be "hclust"
         type = "upper",   # matrix: full, upper, lower
         diag = F,      # remove diagonal
         tl.cex = 0.6,  # font size
         tl.srt = 75, # label angel
         tl.col = "black",
         addrect = 8)
```

## Unimodal Data Visualizations
Let's look at the distribution of individual attributes in the dataset. We'll start with histograms of all of the attributes.

```{r}
# histograms each attribute
par(mfrow=c(3,3))
for(i in 1:9) {
hist(dataset[,i], main=names(dataset)[i])
}
```
We can see that almost all of the distributions have an exponential or bimodal shape to them. We may benefit from log transforms or other power transforms later on.
Let's use density plots to get a more smoothed look at the distributions.

```{r}
# density plot for each attribute
par(mfrow=c(3,3))
complete_cases <- complete.cases(dataset)
for(i in 1:9) {
plot(density(dataset[complete_cases,i]), main=names(dataset)[i])
}
```
These plots add more support to our initial ideas. We can see bimodal distributions (two bumps) and exponential looking distributions.


```{r}
# boxplots for each attribute
par(mfrow=c(3,3))
for(i in 1:9) {
boxplot(dataset[,i], main=names(dataset)[i])
}
```
We see squashed distributions given the exponential shapes we've already observed. Also, because the attributes are scoring of some kind, the scale is limited to [1,10] for all inputs.

## Multimodal Data Visualizations
Now, let's take a look at the interactions between the attributes. Let's start with a scatter plot matrix of the attributes colored by the class values. Because the data is discrete (integer values) we need to add some jitters to make the scatter plot useful, otherwise the dots will all be on top of each other.

```{r}
# scatter plot matrix
jittered_x <- sapply(dataset[,1:9], jitter)
pairs(jittered_x, names(dataset[,1:9]), col=dataset$Class)
```
We can see that the black (benign) apart to be clustered around the bottom-right corner (smaller values) and red (malignant) are all over the place.

Because the data is discrete, we can use bar plots to get an idea of the interaction of the distribution of each attribute and how they breakdown by class value.
```{r}
# bar plots of each variable by class
par(mfrow=c(3,3))
for(i in 1:9) {
  barplot(table(dataset$Class,dataset[,i]), main=names(dataset)[i],
          legend.text=unique(dataset$Class))
}
```
This gives us a more nuanced idea of how the benign values clustered at the left (smaller values) of each distribution and malignant all over the place.


## Evaluate Algorithms - Baseline
 - Linear Algorithms: Logistic Regression (LG), Linear Discriminate Analysis (LDA) and Regularized Logistic Regression (GLMNET).
 - Non-Linear Algorithms: k-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Naive Bayes (NB) and Support Vector Machines with Radial Basis Functions (SVM).

```{r}
library(caret)
set.seed(998)
inTraining <- createDataPartition(dataset$Class, p = .80, list = FALSE)
training <- dataset[ inTraining,]
testing <- dataset[ - inTraining,]
```


```{r}
# 10-fold cross validation with 3 repeats
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
```

```{r}
# View(trainControl)
# library(nlme)
# str(dataset)
```

```{r}
set.seed(7)
fit.glm <- train(Class ~ ., data = dataset, method = "glm", metric = metric, 
                 trControl = trainControl, na.action = na.omit)

# LDA
set.seed(7)
fit.lda <- train(Class~., data=dataset, method="lda", metric=metric, 
                 trControl=trainControl, na.action = na.omit)
# GLMNET
set.seed(7)
fit.glmnet <- train(Class~., data=dataset, method="glmnet", metric=metric,
                    trControl=trainControl, na.action = na.omit)
# KNN
set.seed(7)
fit.knn <- train(Class~., data=dataset, method="knn", metric=metric, 
                 trControl=trainControl, na.action = na.omit)
# CART
set.seed(7)
fit.cart <- train(Class~., data=dataset, method="rpart", metric=metric,
                  trControl=trainControl, na.action = na.omit)
# Naive Bayes
set.seed(7)
fit.nb <- train(Class~., data=dataset, method="nb", metric=metric, 
                trControl=trainControl, na.action = na.omit)
# SVM
set.seed(7)
fit.svm <- train(Class~., data=dataset, method="svmRadial", metric=metric,
                 trControl=trainControl, na.action = na.omit)
# Compare algorithms
results <- resamples(list(LG=fit.glm, LDA=fit.lda, GLMNET=fit.glmnet, KNN=fit.knn,
CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(results)
dotplot(results)
```
We can see good accuracy across the board. All algorithms have a mean accuracy above 90%, well above the baseline of 65% if we just predicted benign. The problem is learnable. We can see that KNN (97.57%) and Naive Bayes (NB was 97.14% and GLMNET was 97.26%) had the highest accuracy on the problem.


## Evaluate Algorithms: Transform
We know we have some skewed distributions. There are transform methods that we can use to adjust and normalize these distributions. A favorite for positive input attributes (which we have in this case) is the Box-Cox transform. In this section we evaluate the same 7 algorithms as above except this time the data is transformed using a Box-Cox power transform to flatten out the distributions.

```{r}
# 10-fold cross validation with 3 repeats
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
# LG
set.seed(7)
fit.glm <- train(Class~., data=dataset, method="glm", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# LDA
set.seed(7)
fit.lda <- train(Class~., data=dataset, method="lda", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# GLMNET
set.seed(7)
fit.glmnet <- train(Class~., data=dataset, method="glmnet", metric=metric,
preProc=c("BoxCox"), trControl=trainControl, na.action = na.omit)
# KNN
set.seed(7)
fit.knn <- train(Class~., data=dataset, method="knn", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# CART
set.seed(7)
fit.cart <- train(Class~., data=dataset, method="rpart", metric=metric,
preProc=c("BoxCox"), trControl=trainControl, na.action = na.omit)
# Naive Bayes
set.seed(7)
fit.nb <- train(Class~., data=dataset, method="nb", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# SVM
set.seed(7)
fit.svm <- train(Class~., data=dataset, method="svmRadial", metric=metric,
preProc=c("BoxCox"), trControl=trainControl, na.action = na.omit)
# Compare algorithms
transformResults <- resamples(list(LG=fit.glm, LDA=fit.lda, GLMNET=fit.glmnet, KNN=fit.knn,
CART=fit.cart, NB=fit.nb, SVM=fit.svm))
summary(transformResults)
dotplot(transformResults)
```
We can see that the accuracy of the previous best algorithm KNN was elevated to 97.63%. 
We have a new ranking, showing SVM with the most accurate mean accuracy at 97.99%.

## Algorithm Tuning
Let's try some tuning of the top algorithms, specically SVM and see if we can lift the accuracy.

### Tuning SVM
The SVM implementation has two parameters that we can tune with caret package. The sigma which is a smoothing term, and C which is a cost constraint. You can learn more about these parameters in the help for the ksvm() function ?ksvm. Let's try a range of values for C between 1 and 10 and a few small values for sigma around the default of 0.1.

```{r}
# 10-fold cross validation with 3 repeats
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
fit.svm <- train(Class~., data=dataset, method="svmRadial", metric=metric, tuneGrid=grid,
preProc=c("BoxCox"), trControl=trainControl, na.action = na.omit)
print(fit.svm)
plot(fit.svm)
```
We can see that we have made very little difference to the results here. The most accurate model had a score of 97.99% (the same as our previously rounded score of 97.99%) using a sigma = 0.50 and C = 1. We could tune further, but I don't expect a payoff.

### Tuning KNN
The KNN implementation has one parameter that we can tune with caret: k the number of closest instances to collect in order to make a prediction. Let's try all k values between 1 and 20.

```{r}
# 10-fold cross validation with 3 repeats
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
set.seed(7)
grid <- expand.grid(.k=seq(1,20,by=1))
fit.knn <- train(Class~., data=dataset, method="knn", metric=metric, tuneGrid=grid,
preProc=c("BoxCox"), trControl=trainControl, na.action = na.omit)
print(fit.knn)
plot(fit.knn)
```
We can see again that tuning has made little difference, settling on a value of k = 19 with an accuracy of 97.75%. This is higher than the previous 97.57%, but very similar (or perhaps identical!) to the result achieved by the tuned SVM.

### Ensemble Methods
As a final check, lets look at some boosting and bagging ensemble algorithms on the dataset. We expect them to do quite well given the decision trees that underlie these methods. If our guess about hitting the accuracy ceiling is true, we may also see these methods top out around 97.20%. Let's look at 4 ensemble methods:
 - Bagging: Bagged CART (BAG) and Random Forest (RF).
 - Boosting: Stochastic Gradient Boosting (GBM) and C5.0 (C50).
We will use the same test harness as before including the Box-Cox transform that fattens out the distributions.

```{r}
# 10-fold cross validation with 3 repeats
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
# Bagged CART
set.seed(7)
fit.treebag <- train(Class~., data=dataset, method="treebag", metric=metric,
trControl=trainControl, na.action = na.omit)
# Random Forest
set.seed(7)
fit.rf <- train(Class~., data=dataset, method="rf", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# Stochastic Gradient Boosting
set.seed(7)
fit.gbm <- train(Class~., data=dataset, method="gbm", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, verbose=FALSE, na.action = na.omit)
# C5.0
set.seed(7)
fit.c50 <- train(Class~., data=dataset, method="C5.0", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, na.action = na.omit)
# Compare results
ensembleResults <- resamples(list(BAG=fit.treebag, RF=fit.rf, GBM=fit.gbm, C50=fit.c50))
summary(ensembleResults)
dotplot(ensembleResults)
```
We see that Random Forest was the most accurate with a score of 97.93%. Very similar to our tuned models above. We could spend time tuning the parameters of random forest (e.g. increasing the number of trees) and the other ensemble methods, but I don't expect to see better accuracy scores other than random statistical fluctuations.

## Finalize Model
We now need to finalize the model, which really means choose which model we would like to use. For simplicity I would probably select the KNN method, at the expense of the memory required to store the training dataset. SVM would be a good choice to trade-off space and time complexity. I probably would not select the Random Forest algorithm given the complexity of the model. It seems overkill for this dataset, lots of trees with little benefit in Accuracy.
Let's go with the KNN algorithm. This is really simple, as we do not need to store a model. We do need to capture the parameters of the Box-Cox transform though. And we also need to prepare the data by removing the unused Id attribute and converting all of the inputs to numeric format. The implementation of KNN (knn3()) belongs to the caret package and does not support missing values. We will have to remove the rows with missing values from the training dataset as well as the validation dataset. The code below shows the preparation of the pre-processing parameters using the training dataset.

```{r}
# prepare parameters for data transform
set.seed(7)
datasetNoMissing <- dataset[complete.cases(dataset),]
x <- datasetNoMissing[,1:9]
preprocessParams <- preProcess(x, method=c("BoxCox"))
x <- predict(preprocessParams, x)
```

Next we need to prepare the validation dataset for making a prediction. We must:
1. Remove the Id attribute.
2. Remove those rows with missing data.
3. Convert all input attributes to numeric.
4. Apply the Box-Cox transform to the input attributes using parameters prepared on the
training dataset.

```{r}
# prepare the validation dataset
set.seed(7)
# remove id column
validation <- validation[,-1]
# remove missing values (not allowed in this implementation of knn)
validation <- validation[complete.cases(validation),]
# convert to numeric
for(i in 1:9) {
validation[,i] <- as.numeric(as.character(validation[,i]))
}
# transform the validation dataset
validationX <- predict(preprocessParams, validation[,1:9])
```

Now we are ready to actually make a prediction in the training dataset.
```{r}
# make predictions
set.seed(7)
predictions <- knn3Train(x, validationX, datasetNoMissing$Class, k=9, prob=FALSE)
confusionMatrix(factor(predictions), 
                factor(validation$Class))
```
We can see that the accuracy of the final model on the validation dataset is 96.3%. This is
optimistic because there is only 135 rows, but it does show that we have an accurate standalone
model that we could use on other unclassified data. However, this final result shows not better than the SVM model with 97.99% accuracy.
